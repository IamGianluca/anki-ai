{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dfc59c0-44ee-4c92-a592-5c7701887fb9",
   "metadata": {},
   "source": [
    "# LLM as a Judge\n",
    "\n",
    "So far, we have been manually reviewing the LLM editor's outputs. This has been a relatively smooth process, but it is not scalable, as there are many failure cases we would need to keep track of. Investing in building an LLM judge makes sense at this stage. \n",
    "\n",
    "Before deploying an LLM judge, we need to ensure its performance is aligned with that of a human judge. This is critical as we would otherwise risk optimizing the wrong metric.\n",
    "\n",
    "Let's get started by creating a small human-annotated dataset of reviews. This dataset will later be used to evaluate the performance of our LLM judge. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2a75b2-92ac-45fc-b93e-7e2c3326f0a2",
   "metadata": {},
   "source": [
    "### Create an Eval dataset\n",
    "\n",
    "To ease the process of creating an eval dataset, we created a small utility class, `ReviewApp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2604a72f-363e-4aa8-8f59-531135520961",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7291c877-6479-4973-b5c9-a6f62d6dbbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from json.decoder import JSONDecodeError\n",
    "from typing import cast\n",
    "\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from anki_ai.domain.model import Deck, Note\n",
    "from anki_ai.entrypoints.review_notes_changes import ReviewApp\n",
    "from anki_ai.service_layer.services import (\n",
    "    ChatCompletionsService,\n",
    "    get_chat_completion,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b53f6d7d-111d-4b1e-852d-b81f8edc16e1",
   "metadata": {},
   "source": [
    "We have collected annotations for over 200 notes. We will use this dataset to evaluate the model's alignment with our preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9c88fe3-541f-49c2-a150-78bc44b96b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "deck = Deck(\"edited\")\n",
    "deck.read_txt(\"../data/new_deck.txt\")\n",
    "ra = ReviewApp(deck=deck)\n",
    "ra.load(\"../data/eval.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8166ca7a-4b17-4645-9547-6e5047bdb116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guid</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A$U26&gt;n14?</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c#*tMdp`:C</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hVkGAdktL6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yyo348j{|9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N1O$1BYpt$</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         guid  score\n",
       "0  A$U26>n14?  False\n",
       "1  c#*tMdp`:C   True\n",
       "2  hVkGAdktL6   True\n",
       "3  yyo348j{|9   True\n",
       "4  N1O$1BYpt$   True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval = pd.read_csv(\"../data/eval.txt\", sep=\"\\t\", header=None)\n",
    "df_eval.columns = [\"guid\", \"score\"]\n",
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df0c191-315e-481a-9aa0-cfe92e427678",
   "metadata": {},
   "source": [
    "### Create a very simple LLM judge\n",
    "\n",
    "Let's create a simple LLM judge, and evaluate its alignment with human preference by measuring how well it does on the eval dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47198333-9083-4c84-82ca-fdf3ccfff28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MSG = r\"\"\"\n",
    "Your job is to evaluate Anki's notes and classify notes that are not formatted correctly.\n",
    "\n",
    "Requirements:\n",
    "* Only check formatting\n",
    "* Notes are written in hybrid markdown; for instance: the newline character is `<br>,` `<` is `&lt;`, etc.\n",
    "* Preserve images and media on the original note\n",
    "* Use code block: ```<language><br><command><br>```\n",
    "* Use inline code format for short commands: e.g., `iw`, `d`, etc.\n",
    "\n",
    "Provide only a boolean score: False for bad and True for good.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def review_note(note: Note, chat: ChatCompletionsService) -> Note:\n",
    "    user_msg = f\"\"\"Front: {note.front}\\nBack: {note.back}\\nTags: {note.tags}\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MSG},\n",
    "        {\"role\": \"user\", \"content\": user_msg},\n",
    "    ]\n",
    "\n",
    "    chat_response = chat.create(\n",
    "        model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "        messages=messages,  # type: ignore\n",
    "        temperature=0,\n",
    "    )\n",
    "    result: str = cast(str, chat_response.choices[0].message.content)\n",
    "\n",
    "    print(user_msg)\n",
    "    print(f\"Eval: {result}\")\n",
    "    return eval(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1aeecd4-cf19-4c31-9d71-0e3deb1f901e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Front: Locker\n",
      "Back: Locker\n",
      "Tags: ['english']\n",
      "Eval: False\n",
      "Ground Truth: False\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Character-level vs word-level tokenization\n",
      "Back: Character-level tokenizers have much smaller vocabularies\n",
      "Tags: ['nlp']\n",
      "Eval: False\n",
      "\n",
      "Reason: The note is missing a newline character after the front and back fields. It should be formatted as:\n",
      "\n",
      "Front: Character-level vs word-level tokenization<br>\n",
      "Back: Character-level tokenizers have much smaller vocabularies<br>\n",
      "Tags: ['nlp']\n",
      "\n",
      "The LLM did not comply with the prompt and returned something different from True or False: invalid syntax (<string>, line 3)\n"
     ]
    }
   ],
   "source": [
    "chat = get_chat_completion()\n",
    "\n",
    "aligned = 0\n",
    "tot = 0\n",
    "try:\n",
    "    for guid, score in ra._ReviewApp__reviews.items():\n",
    "        note = deck.get(guid=guid)[0]\n",
    "        pred = review_note(note, chat)\n",
    "        print(f\"Ground Truth: {score}\\n\")\n",
    "        if pred == score:\n",
    "            aligned += 1\n",
    "        tot += 1\n",
    "        print(\"#######################\\n\")\n",
    "\n",
    "    print(f\"Alignment: {aligned / tot:.2%}\")\n",
    "except SyntaxError as e:\n",
    "    print(\n",
    "        f\"\\nThe LLM did not comply with the prompt and returned something different from True or False: {e}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec69ebec-724d-4b22-bd20-f42c9a12c220",
   "metadata": {},
   "source": [
    "This first model doesn't do well on the task. Let's try to improve it by using structured output and few-shot learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ae7ac6-d84f-43a5-99d9-a798939737c7",
   "metadata": {},
   "source": [
    "### Improve performance of LLM judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fd0d8c-ca82-4573-a82b-46e9996552bf",
   "metadata": {},
   "source": [
    "#### Structured output\n",
    "\n",
    "The LLM judge's performance seems decent, but we should use structured output to make it more manageable and avoid scenarios when the LLM does not follow the instructions properly and returns something other than a boolean. This can happen quite frequently. To address that, let's use structured output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fd89758-9c43-4994-a6e0-5515072a0efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Review(BaseModel):\n",
    "    guid: str\n",
    "    is_correct: bool\n",
    "    # reasoning: str  # TODO: introduce this later and measure uplift\n",
    "\n",
    "\n",
    "def review_note(note: Note, chat: ChatCompletionsService, verbose=False) -> Note:\n",
    "    user_msg = f\"\"\"Front: {note.front}\\nBack: {note.back}\\nTags: {note.tags}\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MSG},\n",
    "        {\"role\": \"user\", \"content\": user_msg},\n",
    "    ]\n",
    "    extra_body = {\n",
    "        \"guided_json\": Review.model_json_schema(),\n",
    "        \"guided_whitespace_pattern\": r\"[\\n\\t ]*\",\n",
    "    }\n",
    "\n",
    "    chat_response = chat.create(\n",
    "        model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "        messages=messages,  # type: ignore\n",
    "        temperature=0,\n",
    "        extra_body=extra_body,\n",
    "    )\n",
    "    content_str: str = cast(str, chat_response.choices[0].message.content)\n",
    "    try:\n",
    "        content_dict = json.loads(content_str)\n",
    "        content_dict[\"guid\"] = note.guid\n",
    "        updated_content_str = json.dumps(content_dict)\n",
    "        result = Review.model_validate_json(updated_content_str)\n",
    "\n",
    "        if verbose:\n",
    "            print(user_msg)\n",
    "            print(f\"Eval: {result}\\n\")\n",
    "\n",
    "        return result\n",
    "    except JSONDecodeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "423215c2-1cf3-44bd-a050-0eadc71df0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Front: Locker\n",
      "Back: Locker\n",
      "Tags: ['english']\n",
      "Eval: guid='A$U26>n14?' is_correct=False\n",
      "\n",
      "Ground Truth: False\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Character-level vs word-level tokenization\n",
      "Back: Character-level tokenizers have much smaller vocabularies\n",
      "Tags: ['nlp']\n",
      "Eval: guid='\"c#*tMdp`:C\"' is_correct=True\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Chipset PCIe lanes name\n",
      "Back: PCH lanes\n",
      "Tags: ['gpu', 'hardware']\n",
      "Eval: guid='hVkGAdktL6' is_correct=True\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: WebSockets vs traditional web communication\n",
      "Back: HTTP follows a request-response model. WebSockets introduce a full-duplex communication channel\n",
      "Tags: ['system-design']\n",
      "Eval: guid='yyo348j{|9' is_correct=True\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Test Time Augmentation\n",
      "Back: At inference/validation time, create multiple versions of each image using data augmentation, then take the average/max of predictions for each version.\n",
      "Tags: ['fastai']\n",
      "Eval: guid='N1O$1BYpt$' is_correct=True\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Left-handed golf stance\n",
      "Back: Feet and knees aligned with target line, weight on left foot, clubface open 10-15 degrees\n",
      "Tags: ['golf']\n",
      "Eval: guid='MPiZ(&lAxK' is_correct=True\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Working Capital use\n",
      "Back: To gauge the short-term health of an organization\n",
      "Tags: ['finance']\n",
      "Eval: guid='J1um1e@6EN' is_correct=True\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: New tab keymap\n",
      "Back: `t`\n",
      "Tags: ['vimium']\n",
      "Eval: guid='cAGU,We~`~' is_correct=True\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Increment HashMap value\n",
      "Back: ```rust<br>match map.get(&w) {<br>    Some(count) => { map.insert(w, count + 1); }<br>    None => { map.insert(w, 1); }<br>}\n",
      "Tags: ['rust-lang']\n",
      "Eval: guid='u,FyN]W/|%' is_correct=True\n",
      "\n",
      "Ground Truth: False\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Open all folds\n",
      "Back: `zR` — Reduce folds (opens all folds)\n",
      "Tags: ['nvim']\n",
      "Eval: guid='\"fC+uI=1h#=\"' is_correct=True\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Assign `s1` to `s2`\n",
      "Back: `s1` is invalidated. Consider using `s1.clone()` instead.\\n```bash\"> cargo run\"> Compiling ownership v0.1.0 (file:///projects/ownership)\"> error[E0382]: borrow of moved value: `s1`\">  --> src/main.rs:5:28\">  |\"> 2 |     let s1 = String::from(\"\"hello\"\");\">  |         -- move occurs because `s1` has type `String`, which does not implement the `Copy` trait\"> 3 |     let s2 = s1;\">  |              -- value moved here\"> 4 |\"> 5 |     println!(\"\"{} world!\"\", s1);\">  |                            ^^ value borrowed here after move\">  |\">  = note: this error originates in the macro `$crate::format_args_nl` which comes from the expansion of the macro `println` (in Nightly builds, run with -Z macro-backtrace for more info)\"> help: consider cloning the value if the performance cost is acceptable\">  |\"> 3 |     let s2 = s1.clone();\">  |                ++++++++\"> For more information about this error, try `rustc --explain E0382`.\"> error: could not compile `ownership` due to previous error\"> ```\n",
      "Tags: ['rust-lang']\n",
      "Eval: guid='sefDNl}8>v' is_correct=True\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Text preprocessing for Transformer model\n",
      "Back: TokenizationNumericalization\n",
      "Tags: ['nlp']\n",
      "Eval: guid='D;FouTT/-X' is_correct=True\n",
      "\n",
      "Ground Truth: False\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: The windshield is steaming up\n",
      "Back: The windshield is fogging up due to high humidity or temperature difference.\n",
      "Tags: ['english']\n",
      "Eval: guid='\"g~#puI)*I%\"' is_correct=True\n",
      "\n",
      "Ground Truth: False\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Get command manual/help\n",
      "Back: ```bash<br>$ man <command><br>```\n",
      "Tags: ['linux']\n",
      "Eval: guid='s=l*N,i*FW' is_correct=True\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: $ \\\\[\\frac{\\partial}{\\partial x} [f(x) - g(x)] = {{c1:: \\frac{\\partial}{\\partial x} f(x) - \\frac{\\partial}{\\partial x} g(x) }} \\]$\n",
      "Back: Difference Rule\n",
      "Tags: ['math']\n",
      "Eval: guid='fqco;Q7@H~' is_correct=True\n",
      "\n",
      "Ground Truth: False\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Brig's favorite food\n",
      "Back: Sandwiches\n",
      "Tags: ['life']\n",
      "Eval: guid='MaS+8AHK*6' is_correct=True\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Accuracy formula\n",
      "Back: $\\\\[math] \\\\[text{ACC} = \\frac{ \\text{TP} + \\text{TN} }{ \\text{TP} + \\text{FP} + \\text{TN} + \\text{FN} } \\[/math] $\n",
      "Tags: ['classification', 'ml']\n",
      "Eval: guid='bd[&6V7P?]' is_correct=True\n",
      "\n",
      "Ground Truth: False\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: DJIA meaning\n",
      "Back: Dow Jones Industrial Average\n",
      "Tags: ['finance']\n",
      "Eval: guid='m%{1B{Q1jr' is_correct=True\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Bayes Theorem conditional probability\n",
      "Back: The posterior\n",
      "Tags: ['stats']\n",
      "Eval: guid='JW|~i]cGSh' is_correct=True\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Jaccard index synonym\n",
      "Back: Intersection over union\n",
      "Tags: ['ml']\n",
      "Eval: guid='MrC556~`Pm' is_correct=True\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: SSL meaning\n",
      "Back: Self-supervised learning\n",
      "Tags: ['ml']\n",
      "Eval: guid='pJ8P[<D3z&' is_correct=True\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Document mimicry\n",
      "Back: A form of prompting which consists in trying to mimic as closely as possible the format of documents that the LLM saw during training (e.g., a conversation transcript)\n",
      "Tags: ['llm']\n",
      "Eval: guid='x43Ud=[8]O' is_correct=True\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Presizing initial validation set resizing\n",
      "Back: The center square of the image is always chosen\n",
      "Tags: ['dl', 'fastai', 'ml']\n",
      "Eval: guid='AY+9%meG%9' is_correct=True\n",
      "\n",
      "Ground Truth: False\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Docker ps meaning\n",
      "Back: [p]rocess [s]tatus\n",
      "Tags: ['docker']\n",
      "Eval: guid='Mmzr]&:9e0' is_correct=True\n",
      "\n",
      "Ground Truth: False\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Open older quickfix list\n",
      "Back: :colder\n",
      "Tags: ['nvim']\n",
      "Eval: guid='b~pDqG>Kbv' is_correct=True\n",
      "\n",
      "Ground Truth: False\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Alignment: 17/25 (68.00%)\n"
     ]
    }
   ],
   "source": [
    "chat = get_chat_completion()\n",
    "\n",
    "aligned = 0\n",
    "tot = 0\n",
    "for guid, score in ra._ReviewApp__reviews.items():\n",
    "    note = deck.get(guid=guid)[0]\n",
    "    pred = review_note(note, chat, verbose=True)\n",
    "    print(f\"Ground Truth: {score}\\n\")\n",
    "    if pred.is_correct == eval(score):\n",
    "        aligned += 1\n",
    "    tot += 1\n",
    "    print(\"#######################\\n\")\n",
    "\n",
    "print(f\"Alignment: {aligned}/{tot} ({aligned / tot:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aaf054-fda7-458e-b97b-e51592169880",
   "metadata": {},
   "source": [
    "#### Few-shot prompting\n",
    "\n",
    "Some of the answers are incorrect. Let's try to pass a few examples to the LLM judge to see if we can improve on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b383fbd-c077-46e4-9c2d-b2412a788d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MSG = r\"\"\"\n",
    "Your job is to evaluate Anki notes, and classify notes that are not formatted correctly.\n",
    "\n",
    "Requirements:\n",
    "* Only check formatting\n",
    "* Notes should be in HTML format; for instance: newline should \"<br>\", \"<\" should be \"&lt;\", etc.\n",
    "* Preserve images and media on the original note\n",
    "* Use code block: ```<language><br><command><br>```\n",
    "* Use inline code format for very short commands: `iw`, `d`, etc.\n",
    "\n",
    "Examples of good notes:\n",
    "\n",
    "Example 1:\n",
    "\n",
    "    Front: Create soft link\n",
    "    Back:  ```bash<br>$ ln -s <file> <link><br>```\n",
    "    Tags:  ['linux']\n",
    "\n",
    "Example 2:\n",
    "\n",
    "    Front: Zip destination option\n",
    "    Back:  ```bash<br>$ unzip <file> -d <path><br>```\n",
    "    Tags:  ['linux']\n",
    "\n",
    "Example 3:\n",
    "\n",
    "    Front: Extract zip files\n",
    "    Back:  ```bash<br>$ unzip <file><br>```\n",
    "    Tags:  ['linux']\n",
    "\n",
    "Example 4:\n",
    "\n",
    "    Front: List directory content\n",
    "    Back:  ```bash<br>$ ls <path><br>```\n",
    "    Tags:  ['linux']\n",
    "\n",
    "Examples of bad notes: \n",
    "\n",
    "Example 1:\n",
    "\n",
    "    Front: Return to previous directory\n",
    "    Back:  ```bash $ cd -```\n",
    "    Tags:  ['linux']\n",
    "\n",
    "    Reasoning: Missing newlines (<br> tags) in code block\n",
    "\n",
    "Example 2: \n",
    "\n",
    "    Front: Remove delimiters\n",
    "    Back:  ```ds <delimiter>```\n",
    "    Tags:  ['nvim']\n",
    "\n",
    "    Reasoning: Using triple backtick quotes without specifying the language and adding newlines (<br> tag) in code block\n",
    "\n",
    "Example 3: \n",
    "\n",
    "    Front: Change Anki delimiters\n",
    "    Back:  ```\\\n",
    "    Tags:  ['nvim']\n",
    "    \n",
    "    Reasoning: Mentioning the command is an Anki command when, in fact, it's a nvim command\n",
    "\n",
    "Example 4: \n",
    "\n",
    "    Front: Text object for a sentence\n",
    "    Back:  ```\\\n",
    "    Tags:  ['nvim']\n",
    "    \n",
    "    Reasoning: Missing command and not closing code block\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def review_note(note: Note, chat: ChatCompletionsService, verbose=False) -> Note:\n",
    "    user_msg = f\"\"\"Front: {note.front}\\nBack: {note.back}\\nTags: {note.tags}\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MSG},\n",
    "        {\"role\": \"user\", \"content\": user_msg},\n",
    "    ]\n",
    "    extra_body = {\n",
    "        \"guided_json\": Review.model_json_schema(),\n",
    "        \"guided_whitespace_pattern\": r\"[\\n\\t ]*\",\n",
    "    }\n",
    "\n",
    "    chat_response = chat.create(\n",
    "        model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "        messages=messages,  # type: ignore\n",
    "        temperature=0,\n",
    "        extra_body=extra_body,\n",
    "    )\n",
    "    content_str: str = cast(str, chat_response.choices[0].message.content)\n",
    "    try:\n",
    "        content_dict = json.loads(content_str)\n",
    "        content_dict[\"guid\"] = note.guid\n",
    "        updated_content_str = json.dumps(content_dict)\n",
    "        result = Review.model_validate_json(updated_content_str)\n",
    "\n",
    "        if verbose:\n",
    "            print(user_msg)\n",
    "            print(f\"Eval: {result}\\n\")\n",
    "\n",
    "        return result\n",
    "    except JSONDecodeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adb4398a-5d79-434b-8207-b789c1f60597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Front: Locker\n",
      "Back: Locker\n",
      "Tags: ['english']\n",
      "Eval: guid='A$U26>n14?' is_correct=False\n",
      "\n",
      "Ground Truth: False\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Character-level vs word-level tokenization\n",
      "Back: Character-level tokenizers have much smaller vocabularies\n",
      "Tags: ['nlp']\n",
      "Eval: guid='\"c#*tMdp`:C\"' is_correct=True\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Chipset PCIe lanes name\n",
      "Back: PCH lanes\n",
      "Tags: ['gpu', 'hardware']\n",
      "Eval: guid='hVkGAdktL6' is_correct=True\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: WebSockets vs traditional web communication\n",
      "Back: HTTP follows a request-response model. WebSockets introduce a full-duplex communication channel\n",
      "Tags: ['system-design']\n",
      "Eval: guid='yyo348j{|9' is_correct=True\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Test Time Augmentation\n",
      "Back: At inference/validation time, create multiple versions of each image using data augmentation, then take the average/max of predictions for each version.\n",
      "Tags: ['fastai']\n",
      "Eval: guid='N1O$1BYpt$' is_correct=True\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Left-handed golf stance\n",
      "Back: Feet and knees aligned with target line, weight on left foot, clubface open 10-15 degrees\n",
      "Tags: ['golf']\n",
      "Eval: guid='MPiZ(&lAxK' is_correct=True\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Working Capital use\n",
      "Back: To gauge the short-term health of an organization\n",
      "Tags: ['finance']\n",
      "Eval: guid='J1um1e@6EN' is_correct=False\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: New tab keymap\n",
      "Back: `t`\n",
      "Tags: ['vimium']\n",
      "Eval: guid='cAGU,We~`~' is_correct=False\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Increment HashMap value\n",
      "Back: ```rust<br>match map.get(&w) {<br>    Some(count) => { map.insert(w, count + 1); }<br>    None => { map.insert(w, 1); }<br>}\n",
      "Tags: ['rust-lang']\n",
      "Eval: guid='u,FyN]W/|%' is_correct=True\n",
      "\n",
      "Ground Truth: False\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Open all folds\n",
      "Back: `zR` — Reduce folds (opens all folds)\n",
      "Tags: ['nvim']\n",
      "Eval: guid='\"fC+uI=1h#=\"' is_correct=False\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Assign `s1` to `s2`\n",
      "Back: `s1` is invalidated. Consider using `s1.clone()` instead.\\n```bash\"> cargo run\"> Compiling ownership v0.1.0 (file:///projects/ownership)\"> error[E0382]: borrow of moved value: `s1`\">  --> src/main.rs:5:28\">  |\"> 2 |     let s1 = String::from(\"\"hello\"\");\">  |         -- move occurs because `s1` has type `String`, which does not implement the `Copy` trait\"> 3 |     let s2 = s1;\">  |              -- value moved here\"> 4 |\"> 5 |     println!(\"\"{} world!\"\", s1);\">  |                            ^^ value borrowed here after move\">  |\">  = note: this error originates in the macro `$crate::format_args_nl` which comes from the expansion of the macro `println` (in Nightly builds, run with -Z macro-backtrace for more info)\"> help: consider cloning the value if the performance cost is acceptable\">  |\"> 3 |     let s2 = s1.clone();\">  |                ++++++++\"> For more information about this error, try `rustc --explain E0382`.\"> error: could not compile `ownership` due to previous error\"> ```\n",
      "Tags: ['rust-lang']\n",
      "Eval: guid='sefDNl}8>v' is_correct=False\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Text preprocessing for Transformer model\n",
      "Back: TokenizationNumericalization\n",
      "Tags: ['nlp']\n",
      "Eval: guid='D;FouTT/-X' is_correct=False\n",
      "\n",
      "Ground Truth: False\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: The windshield is steaming up\n",
      "Back: The windshield is fogging up due to high humidity or temperature difference.\n",
      "Tags: ['english']\n",
      "Eval: guid='\"g~#puI)*I%\"' is_correct=True\n",
      "\n",
      "Ground Truth: False\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Get command manual/help\n",
      "Back: ```bash<br>$ man <command><br>```\n",
      "Tags: ['linux']\n",
      "Eval: guid='s=l*N,i*FW' is_correct=True\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: $ \\\\[\\frac{\\partial}{\\partial x} [f(x) - g(x)] = {{c1:: \\frac{\\partial}{\\partial x} f(x) - \\frac{\\partial}{\\partial x} g(x) }} \\]$\n",
      "Back: Difference Rule\n",
      "Tags: ['math']\n",
      "Eval: guid='fqco;Q7@H~' is_correct=True\n",
      "\n",
      "Ground Truth: False\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Brig's favorite food\n",
      "Back: Sandwiches\n",
      "Tags: ['life']\n",
      "Eval: guid='MaS+8AHK*6' is_correct=True\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Accuracy formula\n",
      "Back: $\\\\[math] \\\\[text{ACC} = \\frac{ \\text{TP} + \\text{TN} }{ \\text{TP} + \\text{FP} + \\text{TN} + \\text{FN} } \\[/math] $\n",
      "Tags: ['classification', 'ml']\n",
      "Eval: guid='bd[&6V7P?]' is_correct=False\n",
      "\n",
      "Ground Truth: False\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: DJIA meaning\n",
      "Back: Dow Jones Industrial Average\n",
      "Tags: ['finance']\n",
      "Eval: guid='m%{1B{Q1jr' is_correct=True\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Bayes Theorem conditional probability\n",
      "Back: The posterior\n",
      "Tags: ['stats']\n",
      "Eval: guid='JW|~i]cGSh' is_correct=False\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Jaccard index synonym\n",
      "Back: Intersection over union\n",
      "Tags: ['ml']\n",
      "Eval: guid='MrC556~`Pm' is_correct=True\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: SSL meaning\n",
      "Back: Self-supervised learning\n",
      "Tags: ['ml']\n",
      "Eval: guid='pJ8P[<D3z&' is_correct=True\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Document mimicry\n",
      "Back: A form of prompting which consists in trying to mimic as closely as possible the format of documents that the LLM saw during training (e.g., a conversation transcript)\n",
      "Tags: ['llm']\n",
      "Eval: guid='x43Ud=[8]O' is_correct=False\n",
      "\n",
      "Ground Truth: True\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Presizing initial validation set resizing\n",
      "Back: The center square of the image is always chosen\n",
      "Tags: ['dl', 'fastai', 'ml']\n",
      "Eval: guid='AY+9%meG%9' is_correct=False\n",
      "\n",
      "Ground Truth: False\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Docker ps meaning\n",
      "Back: [p]rocess [s]tatus\n",
      "Tags: ['docker']\n",
      "Eval: guid='Mmzr]&:9e0' is_correct=True\n",
      "\n",
      "Ground Truth: False\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Front: Open older quickfix list\n",
      "Back: :colder\n",
      "Tags: ['nvim']\n",
      "Eval: guid='b~pDqG>Kbv' is_correct=True\n",
      "\n",
      "Ground Truth: False\n",
      "\n",
      "\n",
      "#######################\n",
      "\n",
      "Alignment: 56.00%\n"
     ]
    }
   ],
   "source": [
    "chat = get_chat_completion()\n",
    "\n",
    "aligned = 0.0\n",
    "tot = 0.0\n",
    "for guid, score in ra._ReviewApp__reviews.items():\n",
    "    note = deck.get(guid=guid)[0]\n",
    "    pred = review_note(note, chat, verbose=True)\n",
    "    print(f\"Ground Truth: {score}\\n\")\n",
    "    if pred.is_correct == eval(score):\n",
    "        aligned += 1\n",
    "    tot += 1\n",
    "    print(\"#######################\\n\")\n",
    "\n",
    "print(f\"Alignment: {aligned / tot:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dc74d3-95ef-4322-a8ea-701f5942e859",
   "metadata": {},
   "source": [
    "This result is also surprising. We would have expected a few examples to help the model understand what is the expected formatting for these notes. \n",
    "\n",
    "A few things we want to try next: \n",
    "1. For each type of common error (e.g., double backslash on LaTeX code, code block for math, etc.), provide both a negative and positive example\n",
    "1. Ask the LLM to provide reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017974df-4813-447e-8b53-ee6fae73c622",
   "metadata": {},
   "source": [
    "### Create helper functions to facilitate reviewing notes\n",
    "\n",
    "Let's create a `pandas.DataFrame` with both: original note, edited note, and LLM review. This will facilitate our review of the LLM reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bf1acc2-cd57-43bd-8eb5-ddc7751151b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dict_data \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mdict() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresults\u001b[49m]\n\u001b[1;32m      2\u001b[0m df_scores \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(dict_data)\n\u001b[1;32m      3\u001b[0m df_scores\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "dict_data = [item.dict() for item in results]\n",
    "df_scores = pd.DataFrame(dict_data)\n",
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f345c24-71b9-4b66-9851-a41df05a967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [note.dict() for note in deck]\n",
    "df_notes = pd.DataFrame(a)\n",
    "df_notes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefe3d5f-5f7e-4004-a8be-0a041b18f87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.merge(df_notes, df_scores, how=\"inner\", on=\"guid\")\n",
    "x = x[x.tags.apply(lambda a: \"life\" not in a)]  # exclude personal notes\n",
    "print(x.shape)\n",
    "x.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67017da-94d2-41bf-b141-3f59f5caddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_interactive_session(session_text):\n",
    "    lines = session_text.strip().split(\"<br>\")\n",
    "    input_pattern = r\"^>>> .*$\"\n",
    "    continuation_pattern = r\"^... .*$\"\n",
    "    output_pattern = r\"^(?!>>>)(?!\\.\\.\\.)\"\n",
    "\n",
    "    state = \"expecting_input\"\n",
    "    for i, line in enumerate(lines, 1):\n",
    "        if state == \"expecting_input\":\n",
    "            if not (\n",
    "                re.match(input_pattern, line) or re.match(continuation_pattern, line)\n",
    "            ):\n",
    "                return False, f\"Line {i}: Expected input (>>> or ...), got: {line}\"\n",
    "            state = \"optional_output\"\n",
    "        elif state == \"optional_output\":\n",
    "            if re.match(input_pattern, line) or re.match(continuation_pattern, line):\n",
    "                state = \"expecting_input\"\n",
    "            elif not re.match(output_pattern, line):\n",
    "                return False, f\"Line {i}: Invalid output format: {line}\"\n",
    "\n",
    "    return True, \"Valid interactive session format\"\n",
    "\n",
    "\n",
    "def validate_code_block_format(block):\n",
    "    # Check if the block starts and ends with ```\n",
    "    if not (block.startswith(\"```\") and block.endswith(\"```\")):\n",
    "        return False, \"Code block should start and end with ```\"\n",
    "\n",
    "    # Remove the opening and closing ```\n",
    "    content = block[3:-3].strip()\n",
    "\n",
    "    # Check if the block starts with a language specifier\n",
    "    if not re.match(r\"^[\\w-]+<br>\", content):\n",
    "        return (\n",
    "            False,\n",
    "            \"Code block should start with a language specifier followed by <br>\",\n",
    "        )\n",
    "\n",
    "    # Split the content by <br> tags\n",
    "    lines = content.split(\"<br>\")\n",
    "\n",
    "    # Check if the last line is empty (as it should end with <br>)\n",
    "    if lines[-1].strip() != \"\":\n",
    "        return False, \"Code block should end with <br>\"\n",
    "\n",
    "    # Check if there are any empty lines in between (which would indicate missing <br>)\n",
    "    if any(line.strip() == \"\" for line in lines[1:-1]):\n",
    "        return (\n",
    "            False,\n",
    "            \"Code block should not have empty lines. Use <br> for line breaks.\",\n",
    "        )\n",
    "\n",
    "    return True, \"Valid code block format\"\n",
    "\n",
    "\n",
    "def validate_hybrid_markdown(content):\n",
    "    issues = []\n",
    "\n",
    "    # Check for double backslashes in LaTeX blocks\n",
    "    latex_blocks = re.findall(r\"\\$(.*?)\\$\", content, re.DOTALL)\n",
    "    for block in latex_blocks:\n",
    "        if \"\\\\\\\\\" in block:\n",
    "            issues.append(\n",
    "                \"Double backslash (\\\\\\\\) found in LaTeX block. This may cause rendering issues.\"\n",
    "            )\n",
    "\n",
    "    # Check for unmatched dollar signs\n",
    "    # Split the content into code blocks and non-code blocks\n",
    "    parts = re.split(r\"(```[\\s\\S]*?```)\", content)\n",
    "\n",
    "    total_dollar_count = 0\n",
    "    for part in parts:\n",
    "        if part.startswith(\"```\") and part.endswith(\"```\"):\n",
    "            # This is a code block\n",
    "            is_valid, message = validate_code_block_format(part)\n",
    "            if not is_valid:\n",
    "                issues.append(f\"Invalid code block format: {message}\")\n",
    "\n",
    "            if part.startswith(\"```python\"):\n",
    "                # Check if it's an interactive Python session\n",
    "                session_content = part[13:-3].strip()  # Remove ```python<br> and ```\n",
    "                is_valid, message = validate_interactive_session(session_content)\n",
    "                if not is_valid:\n",
    "                    issues.append(\n",
    "                        f\"Invalid Python interactive session in code block: {message}\"\n",
    "                    )\n",
    "        else:\n",
    "            # Count dollar signs in non-code block parts\n",
    "            dollar_count = part.count(\"$\")\n",
    "            total_dollar_count += dollar_count\n",
    "\n",
    "    # Check if the total number of dollar signs outside code blocks is odd\n",
    "    if total_dollar_count % 2 != 0:\n",
    "        issues.append(\n",
    "            \"Unmatched dollar signs outside code blocks. LaTeX may not render correctly.\"\n",
    "        )\n",
    "\n",
    "    # Check for common Markdown syntax errors\n",
    "    if \"```\" in content and content.count(\"```\") % 2 != 0:\n",
    "        issues.append(\n",
    "            \"Unmatched code block delimiters (```). Code blocks may not render correctly.\"\n",
    "        )\n",
    "\n",
    "    return issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de65941-7107-4a17-88cc-3590d0f1ca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reviews = 100\n",
    "\n",
    "for row in x.iloc[:n_reviews].iterrows():\n",
    "    note = row[1]\n",
    "    print(f\"Front: {note['front']}\\nBack: {note['back']}\\nTags: {note['tags']}\")\n",
    "    for side in [\"front\", \"back\"]:\n",
    "        a = note[side]\n",
    "        issues = validate_hybrid_markdown(a)\n",
    "        if issues:\n",
    "            for issue in issues:\n",
    "                print(f\"Issue {side}: {issue}\")\n",
    "        else:\n",
    "            print(f\"Issue {side}: None\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba9168c-46f6-49a7-851b-4b23a9669aff",
   "metadata": {},
   "source": [
    "Common errors are:\n",
    "\n",
    "* Missing `<img>`\n",
    "* Wrong prompt (e.g., `>>`, missing `$`)\n",
    "* Missing `<br>` inside code block\n",
    "* Missing `<br>` outside code block\n",
    "* `\\\\` in LaTeX\n",
    "* References (should we remove them?)\n",
    "* Trailing `.` (full stop)\n",
    "* Using code block for note that does not contain code\n",
    "* \"```bash\" for keymap\n",
    "* Missing language in code block\n",
    "* Unmatched code block delimiter (missing trailing \"```\")\n",
    "* Missing inline code block for keymap or short commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0fc84d-92e4-4f4a-a84f-cd96d9921e5b",
   "metadata": {},
   "source": [
    "### Todo\n",
    "\n",
    "- [ ] Create a dataset to measure LLM judge's alignment with human preference \n",
    "- [ ] Use _reflection_ agentic workflow to improve notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096944a4-fb5c-47b1-ac86-88ac99949927",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
