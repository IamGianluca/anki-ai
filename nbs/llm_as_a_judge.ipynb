{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dfc59c0-44ee-4c92-a592-5c7701887fb9",
   "metadata": {},
   "source": [
    "# LLM as a Judge\n",
    "\n",
    "So far, we have been manually reviewing the LLM editor's outputs. This has been a relatively smooth process, but it is not scalable, as there are many failure cases we would need to keep track of. Investing in building an LLM judge makes sense at this stage. \n",
    "\n",
    "Before deploying an LLM judge, we need to ensure its performance is aligned with that of a human judge. This is critical as we would otherwise risk optimizing the wrong metric.\n",
    "\n",
    "Let's get started by creating a small human-annotated dataset of reviews. This dataset will later be used to evaluate the performance of our LLM judge. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2a75b2-92ac-45fc-b93e-7e2c3326f0a2",
   "metadata": {},
   "source": [
    "### Create an Eval dataset\n",
    "\n",
    "To ease the process of creating an eval dataset, we built a small utility class, `ReviewApp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f123a989-6be4-47b6-88d3-4d0bc6b8ab0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m244 packages\u001b[0m \u001b[2min 0.55ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m162 packages\u001b[0m \u001b[2min 0.06ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2604a72f-363e-4aa8-8f59-531135520961",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7291c877-6479-4973-b5c9-a6f62d6dbbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import namedtuple\n",
    "from json.decoder import JSONDecodeError\n",
    "from typing import cast\n",
    "\n",
    "import pandas as pd\n",
    "from jinja2 import Template\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from anki_ai.domain.deck import Deck\n",
    "from anki_ai.entrypoints.format_notes import ReviewApp\n",
    "from anki_ai.service_layer.services import (\n",
    "    ChatCompletionsService,\n",
    "    get_chat_completion,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b53f6d7d-111d-4b1e-852d-b81f8edc16e1",
   "metadata": {},
   "source": [
    "We have collected annotations for over 200 notes. We will use this dataset to evaluate the model's alignment with our preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "387bd2a9-04cb-44f3-b53e-2d1ae92e7233",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/new_deck.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m orig_deck\u001b[38;5;241m.\u001b[39mread_txt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/Selected Notes v8.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m deck \u001b[38;5;241m=\u001b[39m Deck()\n\u001b[0;32m----> 5\u001b[0m \u001b[43mdeck\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_txt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/new_deck.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/anki-ai/src/anki_ai/domain/deck.py:46\u001b[0m, in \u001b[0;36mDeck.read_txt\u001b[0;34m(self, fpath, exclude_tags)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_txt\u001b[39m(\u001b[38;5;28mself\u001b[39m, fpath: Path, exclude_tags: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(f):\n\u001b[1;32m     48\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/new_deck.txt'"
     ]
    }
   ],
   "source": [
    "orig_deck = Deck()\n",
    "orig_deck.read_txt(\"../data/Selected Notes v8.txt\")\n",
    "\n",
    "deck = Deck()\n",
    "deck.read_txt(\"../data/new_deck.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9777f93-d89f-4f60-b698-abb5e184052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra = ReviewApp(old_deck=orig_deck, new_deck=deck)\n",
    "ra.load(\"../data/eval.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df0c191-315e-481a-9aa0-cfe92e427678",
   "metadata": {},
   "source": [
    "### Create a very simple LLM judge\n",
    "\n",
    "Let's create a simple LLM judge, and evaluate its alignment with human preference by measuring how well it does on the eval dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47198333-9083-4c84-82ca-fdf3ccfff28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MSG_V1 = r\"\"\"\n",
    "Your job is to evaluate Anki's notes and classify notes that are not formatted correctly.\n",
    "\n",
    "Requirements:\n",
    "* Only check formatting\n",
    "* Notes are written in hybrid markdown; for instance: the newline character is `<br>,` `<` is `&lt;`, etc.\n",
    "* Preserve images and media on the original note\n",
    "* Use code block: ```<language><br><command><br>```\n",
    "* Use inline code format for short commands: e.g., `iw`, `d`, etc.\n",
    "\n",
    "Provide only a boolean score: False for bad and True for good.\n",
    "\"\"\"\n",
    "\n",
    "# MODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-14B-Instruct-AWQ\"\n",
    "# MODEL_NAME = \"Qwen/Qwen2.5-32B-Instruct-AWQ\"\n",
    "\n",
    "\n",
    "class LLMJudge:\n",
    "    def __init__(\n",
    "        self,\n",
    "        chat: ChatCompletionsService,\n",
    "        system_msg: str,\n",
    "        user_msg_tmpl: str,\n",
    "        model_name: str = \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    ) -> None:\n",
    "        self.chat = chat\n",
    "        self.system_msg = system_msg\n",
    "        self.user_msg_tmpl = Template(user_msg_tmpl)\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def review(self, note, verbose=False) -> bool:\n",
    "        user_msg = self.user_msg_tmpl.render(note=note)\n",
    "        if verbose:\n",
    "            print(user_msg)\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "        ]\n",
    "        chat_response = self.chat.create(\n",
    "            model=self.model_name,\n",
    "            messages=messages,  # type: ignore\n",
    "            temperature=0,\n",
    "        )\n",
    "        result: str = cast(str, chat_response.choices[0].message.content)\n",
    "        return Verdict(is_correct=eval(result))\n",
    "\n",
    "\n",
    "user_msg_tmpl = \"\"\"Front: {{ note.front }}\n",
    "Back: {{ note.back }}\n",
    "Tags: {{ note.tags }}\n",
    "\"\"\"\n",
    "\n",
    "chat = get_chat_completion()\n",
    "judge = LLMJudge(\n",
    "    model_name=MODEL_NAME,\n",
    "    chat=chat,\n",
    "    system_msg=SYSTEM_MSG_V1,\n",
    "    user_msg_tmpl=user_msg_tmpl,\n",
    ")\n",
    "Verdict = namedtuple(\"Verdict\", [\"is_correct\"])\n",
    "\n",
    "\n",
    "def review_notes(ra, judge, verbose=False):\n",
    "    aligned = 0\n",
    "    tot = 0\n",
    "    for guid, score in ra._ReviewApp__reviews.items():\n",
    "        note = deck.get(guid=guid)[0]\n",
    "        try:\n",
    "            verdict = judge.review(note=note, verbose=verbose)\n",
    "            if verdict.is_correct == eval(score):\n",
    "                aligned += 1\n",
    "            tot += 1\n",
    "            print(f\"Ground Truth: {eval(score)}\\nVerdict: {verdict}\")\n",
    "        except SyntaxError as e:\n",
    "            print(\n",
    "                f\"The LLM did not comply with the prompt and returned something different from True or False: {e}\"\n",
    "            )\n",
    "            print(\"#######################\")\n",
    "        except AttributeError as e:\n",
    "            print(f\"The LLM did not comply with the prompt and did not fill the `: {e}\")\n",
    "            print(\"#######################\")\n",
    "        else:\n",
    "            print(\"#######################\")\n",
    "    print(f\"Alignment: {aligned}/{tot} ({aligned / tot:.2%})\")\n",
    "\n",
    "\n",
    "review_notes(ra=ra, judge=judge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec69ebec-724d-4b22-bd20-f42c9a12c220",
   "metadata": {},
   "source": [
    "This first model is not that bad; however, quite frequently, it does not follow the instructions and returns something other than a boolean. Let's fix that by using structured output and improving performance with some prompt engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ae7ac6-d84f-43a5-99d9-a798939737c7",
   "metadata": {},
   "source": [
    "### Improve performance of LLM judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fd0d8c-ca82-4573-a82b-46e9996552bf",
   "metadata": {},
   "source": [
    "#### Structured output\n",
    "\n",
    "The LLM judge's performance seems decent, but we should use structured output to make it more manageable and avoid scenarios when the LLM does not follow the instructions properly and returns something other than a boolean. This can happen quite frequently. To address that, let's use structured output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd89758-9c43-4994-a6e0-5515072a0efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Review(BaseModel):\n",
    "    guid: str\n",
    "    is_correct: bool\n",
    "\n",
    "\n",
    "class LLMJudgeJSON:\n",
    "    def __init__(\n",
    "        self,\n",
    "        chat: ChatCompletionsService,\n",
    "        system_msg: str,\n",
    "        user_msg_tmpl: str,\n",
    "        model_name: str = \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "        review_model=Review,\n",
    "    ) -> None:\n",
    "        self.chat = chat\n",
    "        self.system_msg = system_msg\n",
    "        self.user_msg_tmpl = Template(user_msg_tmpl)\n",
    "        self.model_name = model_name\n",
    "        self.review_model = review_model\n",
    "\n",
    "    def review(self, note, verbose=False) -> bool:\n",
    "        user_msg = self.user_msg_tmpl.render(note=note)\n",
    "        if verbose:\n",
    "            print(user_msg)\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "        ]\n",
    "        extra_body = {\n",
    "            \"guided_json\": self.review_model.model_json_schema(),\n",
    "            \"guided_whitespace_pattern\": r\"[\\n\\t ]*\",\n",
    "        }\n",
    "\n",
    "        chat_response = chat.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=messages,  # type: ignore\n",
    "            temperature=0,\n",
    "            extra_body=extra_body,\n",
    "        )\n",
    "        content_str: str = cast(str, chat_response.choices[0].message.content)\n",
    "        try:\n",
    "            content_dict = json.loads(content_str)\n",
    "            content_dict[\"guid\"] = note.guid\n",
    "            updated_content_str = json.dumps(content_dict)\n",
    "            result = self.review_model.model_validate_json(updated_content_str)\n",
    "            return result\n",
    "        except JSONDecodeError as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "judge_json = LLMJudgeJSON(\n",
    "    model_name=MODEL_NAME,\n",
    "    chat=chat,\n",
    "    system_msg=SYSTEM_MSG_V1,\n",
    "    user_msg_tmpl=user_msg_tmpl,\n",
    ")\n",
    "review_notes(ra=ra, judge=judge_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aaf054-fda7-458e-b97b-e51592169880",
   "metadata": {},
   "source": [
    "#### Few-shot prompting\n",
    "\n",
    "Some of the answers are incorrect. Let's try to pass a few examples to the LLM judge to see if we can improve on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b383fbd-c077-46e4-9c2d-b2412a788d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MSG_V2 = r\"\"\"\n",
    "Your job is to evaluate Anki notes, and classify notes that are not formatted correctly.\n",
    "\n",
    "Requirements:\n",
    "* Only check formatting\n",
    "* Notes should be in HTML format; for instance: newline should \"<br>\", \"<\" should be \"&lt;\", etc.\n",
    "* Preserve images and media on the original note\n",
    "* Use code block: ```<language><br><command><br>```\n",
    "* Use inline code format for very short commands: `iw`, `d`, etc.\n",
    "\n",
    "Examples of good notes:\n",
    "\n",
    "Example 1:\n",
    "\n",
    "    Front: Create soft link\n",
    "    Back:  ```bash<br>$ ln -s <file> <link><br>```\n",
    "    Tags:  ['linux']\n",
    "\n",
    "Example 2:\n",
    "\n",
    "    Front: Zip destination option\n",
    "    Back:  ```bash<br>$ unzip <file> -d <path><br>```\n",
    "    Tags:  ['linux']\n",
    "\n",
    "Example 3:\n",
    "\n",
    "    Front: Extract zip files\n",
    "    Back:  ```bash<br>$ unzip <file><br>```\n",
    "    Tags:  ['linux']\n",
    "\n",
    "Example 4:\n",
    "\n",
    "    Front: List directory content\n",
    "    Back:  ```bash<br>$ ls <path><br>```\n",
    "    Tags:  ['linux']\n",
    "\n",
    "Examples of bad notes: \n",
    "\n",
    "Example 1:\n",
    "\n",
    "    Front: Return to previous directory\n",
    "    Back:  ```bash $ cd -```\n",
    "    Tags:  ['linux']\n",
    "\n",
    "    Reasoning: Missing newlines (<br> tags) in code block\n",
    "\n",
    "Example 2: \n",
    "\n",
    "    Front: Remove delimiters\n",
    "    Back:  ```ds <delimiter>```\n",
    "    Tags:  ['nvim']\n",
    "\n",
    "    Reasoning: Using triple backtick quotes without specifying the language and adding newlines (<br> tag) in code block\n",
    "\n",
    "Example 3: \n",
    "\n",
    "    Front: Change Anki delimiters\n",
    "    Back:  ```\\\n",
    "    Tags:  ['nvim']\n",
    "    \n",
    "    Reasoning: Mentioning the command is an Anki command when, in fact, it's a nvim command\n",
    "\n",
    "Example 4: \n",
    "\n",
    "    Front: Text object for a sentence\n",
    "    Back:  ```\\\n",
    "    Tags:  ['nvim']\n",
    "    \n",
    "    Reasoning: Missing command and not closing code block\n",
    "\"\"\"\n",
    "\n",
    "judge_json = LLMJudgeJSON(\n",
    "    model_name=MODEL_NAME,\n",
    "    chat=chat,\n",
    "    system_msg=SYSTEM_MSG_V2,\n",
    "    user_msg_tmpl=user_msg_tmpl,\n",
    ")\n",
    "review_notes(ra=ra, judge=judge_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4c7cea-f500-45a5-9bf1-0c47e5b5e206",
   "metadata": {},
   "source": [
    "This result is also surprising. We would have expected a few examples to help the model understand the expected formatting for these notes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6cb930-60d6-4226-abf5-4efe19091b87",
   "metadata": {},
   "source": [
    "#### Reasoning\n",
    "\n",
    "One thing we would expect to improve performance is to ask the LLM judge to provide some reasoning for its decision before submitting a verdict. Let's see if that works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84716b2d-34d7-4a83-8a83-cafb5669535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MSG_V3 = r\"\"\"\n",
    "Your job is to evaluate the formatting of Anki note.\n",
    "\n",
    "Properly formatted notes should:\n",
    "* Use hybrid markdown format. For instance, use \"<br>\" to signal a new line, \"&lt;\" for \"<\" symbol, etc.\n",
    "* Preserve images and media on the original note\n",
    "* Wrap code in a code block: ```<language><br><command><br>```\n",
    "* Wrap math in a LaTeX block: $ <math equation> $. Also, ensure that we do not use double backslashes, \\\\, in a LaTeX block, as that won't be correctly displayed\n",
    "* Wrap short commands in an inline code block: `iw`, `d`, etc.\n",
    "\n",
    "Provide concise reasoning for your answer and a True/False answer, where True means the note is formatted correctly and False means the note is not properly formatted.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Review2(BaseModel):\n",
    "    guid: str\n",
    "    reasoning: str\n",
    "    is_correct: bool\n",
    "\n",
    "\n",
    "judge_json = LLMJudgeJSON(\n",
    "    model_name=MODEL_NAME,\n",
    "    chat=chat,\n",
    "    system_msg=SYSTEM_MSG_V3,\n",
    "    user_msg_tmpl=user_msg_tmpl,\n",
    "    review_model=Review2,\n",
    ")\n",
    "review_notes(ra=ra, judge=judge_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b345368e-26f6-4e5c-9e3e-fd2c08d7ba05",
   "metadata": {},
   "source": [
    "#### Provide original notes\n",
    "\n",
    "The LLM judge made some mistakes due to not having access to the original note. For instance, the LLM judge would not know if the LLM editor removed an image or block of code present in the original note. Let's try to address that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca9f553-afb1-4cf5-8ef8-a93e7ba53571",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MSG_V4 = r\"\"\"\n",
    "The user will share two Anki notes: the original and improved versions. The improved version should be factually the same as the original note but more concise and might have a slightly different format.\n",
    "\n",
    "Your job is to evaluate the formatting of the improved version. Properly formatted notes should:\n",
    "* Use hybrid markdown format. For instance, when relevant, use `<br>` to signal a new line, `&nbsp;` to signal a non-breaking space, `&lt;` for `<` symbol, etc. Cards with just one sentence that do not include code or math equations do not require special formatting\n",
    "* Preserve images and media present on the original note\n",
    "* Code should be wrapped in a code block: ```<language><br><command><br>```. One line command should use an inline code block: `iw`, `d`, `:copen`, etc.\n",
    "* Mathematical equations should be wrapped in a LaTeX block: $ <math equation> $. Also, ensure that we do not use double backslashes, \\\\, in a LaTeX block, as that won't be correctly displayed\n",
    "\n",
    "Provide concise reasoning (no more than two sentences) for your answer and a True/False answer, where True means the improved note is formatted correctly and False means the improved note is not properly formatted.\n",
    "\"\"\"\n",
    "\n",
    "user_msg_tmpl_2 = \"\"\"Original note\n",
    "Front: {{ orig_note.front }}\n",
    "Back: {{ orig_note.back }}\n",
    "Tags: {{ orig_note.tags }}\n",
    "\n",
    "Improved note\n",
    "Front: {{ note.front }}\n",
    "Back: {{ note.back }}\n",
    "Tags: {{ note.tags }}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class LLMJudgeJSON:\n",
    "    def __init__(\n",
    "        self,\n",
    "        chat: ChatCompletionsService,\n",
    "        system_msg: str,\n",
    "        user_msg_tmpl: str,\n",
    "        orig_deck: Deck,\n",
    "        model_name: str = \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "        review_model=Review,\n",
    "    ) -> None:\n",
    "        self.chat = chat\n",
    "        self.system_msg = system_msg\n",
    "        self.user_msg_tmpl = Template(user_msg_tmpl)\n",
    "        self.model_name = model_name\n",
    "        self.review_model = review_model\n",
    "        self.orig_deck = orig_deck\n",
    "\n",
    "    def review(self, note, verbose=False) -> bool:\n",
    "        orig_note = self.orig_deck.get(note.guid)[0]\n",
    "        user_msg = self.user_msg_tmpl.render(orig_note=orig_note, note=note)\n",
    "        if verbose:\n",
    "            print(user_msg)\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "        ]\n",
    "        extra_body = {\n",
    "            \"guided_json\": self.review_model.model_json_schema(),\n",
    "            \"guided_whitespace_pattern\": r\"[\\n\\t ]*\",\n",
    "        }\n",
    "\n",
    "        chat_response = chat.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=messages,  # type: ignore\n",
    "            temperature=0,\n",
    "            extra_body=extra_body,\n",
    "        )\n",
    "        content_str: str = cast(str, chat_response.choices[0].message.content)\n",
    "        try:\n",
    "            content_dict = json.loads(content_str)\n",
    "            content_dict[\"guid\"] = note.guid\n",
    "            updated_content_str = json.dumps(content_dict)\n",
    "            result = self.review_model.model_validate_json(updated_content_str)\n",
    "            return result\n",
    "        except JSONDecodeError as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "judge_json2 = LLMJudgeJSON(\n",
    "    model_name=MODEL_NAME,\n",
    "    chat=chat,\n",
    "    system_msg=SYSTEM_MSG_V4,\n",
    "    user_msg_tmpl=user_msg_tmpl_2,\n",
    "    review_model=Review2,\n",
    "    orig_deck=orig_deck,\n",
    ")\n",
    "review_notes(ra=ra, judge=judge_json2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280a996d-da73-4f5e-bebe-40df05ddba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MSG_V5 = r\"\"\"\n",
    "The user will share two Anki notes: the original and improved versions. Here is an example of the input:\n",
    "\n",
    "Original note:\n",
    "Front: <original front>\n",
    "Back: <original back>\n",
    "Tags: <original tags>\n",
    "\n",
    "Improved note:\n",
    "Front: <improved front>\n",
    "Back: <improved back>\n",
    "Tags: <improved tags>\n",
    "\n",
    "The improved version should be factually the same as the original note but more concise and might have a slightly different format.\n",
    "\n",
    "Evaluate the formatting of the improved note, both front and back cards. Properly formatted notes should:\n",
    "* Use hybrid markdown format. For instance, when relevant, use `<br>` to signal a new line, `&lt;` for `<` symbol, etc. Cards with just one sentence that do not include code or math equations do not require special formatting\n",
    "* Preserve images and media present on the original note\n",
    "* Code should be wrapped in a code block: ```<language><br><command><br>```. One line command should use an inline code block: `iw`, `d`, `:copen`, etc.\n",
    "* Mathematical equations should be wrapped in a LaTeX block: $ <math equation> $. Also, ensure that we do not use double backslashes, \\\\, in a LaTeX block, as that won't be correctly displayed\n",
    "\n",
    "The original note is provided only as a reference to ensure we are preserving the intention of the note and any media/code example.\n",
    "\n",
    "Respond using JSON format with two fields:\n",
    "* \"reasoning\": A concise explanation for your answer.\n",
    "* \"is_correct\": Whether the improved note is formatted correctly (True) or not (False).\n",
    "\"\"\"\n",
    "\n",
    "judge_json2 = LLMJudgeJSON(\n",
    "    model_name=MODEL_NAME,\n",
    "    chat=chat,\n",
    "    system_msg=SYSTEM_MSG_V5,\n",
    "    user_msg_tmpl=user_msg_tmpl_2,\n",
    "    review_model=Review2,\n",
    "    orig_deck=orig_deck,\n",
    ")\n",
    "review_notes(ra=ra, judge=judge_json2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aabe5b6-3201-431a-8e02-f309b101125b",
   "metadata": {},
   "source": [
    "It seems the LLM judge is often confusing the old and new notes. Let's try to refactor the prompt (thank you Claude)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2631d493-4748-4c05-aed6-cd8bd3e0c092",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MSG_V6 = r\"\"\"You will be presented with two versions of an Anki note: the original and an improved version. Your task is to evaluate the formatting of the improved note, ensuring it maintains the original's factual content while potentially being more concise or having a slightly different format.\n",
    "\n",
    "## Input Format\n",
    "\n",
    "The input will be structured as follows:\n",
    "\n",
    "```\n",
    "Original note:\n",
    "Front: <original front content>\n",
    "Back: <original back content>\n",
    "Tags: <original tags>\n",
    "\n",
    "Improved note:\n",
    "Front: <improved front content>\n",
    "Back: <improved back content>\n",
    "Tags: <improved tags>\n",
    "```\n",
    "\n",
    "## Evaluation Criteria\n",
    "\n",
    "Assess the improved note's formatting for both front and back sides. A properly formatted note should:\n",
    "\n",
    "1. Utilize hybrid markdown format:\n",
    "   - Use `<br>` for line breaks when necessary\n",
    "   - Use `&lt;` for `<` symbol, `&gt;` for `>`, etc.\n",
    "   - Simple cards with a single sentence and no code/math may not require special formatting\n",
    "\n",
    "2. Preserve all images and media from the original note\n",
    "\n",
    "3. Format code correctly:\n",
    "   - Multi-line code: Use code blocks with language specification\n",
    "     ```<language>\n",
    "     <code>\n",
    "     ```\n",
    "   - Single-line commands: Use inline code blocks, e.g., `command`\n",
    "\n",
    "4. Format mathematical equations properly:\n",
    "   - Wrap in LaTeX blocks: $ <equation> $\n",
    "   - Avoid double backslashes (\\\\) within LaTeX blocks\n",
    "\n",
    "5. Maintain the original note's intent and key information\n",
    "\n",
    "## Output Format\n",
    "\n",
    "Provide your evaluation as follows:\n",
    "\n",
    "1. A concise reasoning for your assessment, highlighting any formatting issues or improvements\n",
    "2. A boolean verdict: \n",
    "   - `True` if the improved note is correctly formatted\n",
    "   - `False` if the improved note has formatting issues\n",
    "\n",
    "Example output:\n",
    "```\n",
    "Reasoning: The improved note correctly uses hybrid markdown, preserves images, and properly formats code and equations. Line breaks are appropriately handled with <br> tags.\n",
    "\n",
    "Verdict: True\n",
    "```\n",
    "\n",
    "Remember, the original note serves as a reference to ensure the improved version preserves the intended content and any necessary media or code examples.\n",
    "\"\"\"\n",
    "\n",
    "judge_json2 = LLMJudgeJSON(\n",
    "    model_name=MODEL_NAME,\n",
    "    chat=chat,\n",
    "    system_msg=SYSTEM_MSG_V6,\n",
    "    user_msg_tmpl=user_msg_tmpl_2,\n",
    "    review_model=Review2,\n",
    "    orig_deck=orig_deck,\n",
    ")\n",
    "review_notes(ra=ra, judge=judge_json2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fa34df-c8c5-4e89-8324-d153458a15a2",
   "metadata": {},
   "source": [
    "Let's see if Meta AI can help here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ff80ce-6650-4912-8f87-fb78628fa084",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MSG_V7 = r\"\"\"# Anki Note Formatting Evaluation\n",
    "Provide two Anki notes:\n",
    "\n",
    "Original Note\n",
    "Front: <original front>\n",
    "Back: <original back>\n",
    "Tags: <original tags>\n",
    "\n",
    "Improved Note\n",
    "Front: <improved front>\n",
    "Back: <improved back>\n",
    "Tags: <improved tags>\n",
    "\n",
    "# Evaluation Criteria\n",
    "Assess the formatting of the improved note, ensuring it:\n",
    "1. Uses hybrid markdown format (e.g., <br> for new lines, &lt; for < symbol)\n",
    "2. Preserves images and media from the original note\n",
    "3. Formats code using:\n",
    "  * Code blocks (<language><br><command><br>) for multiple lines\n",
    "  * Inline code blocks () for single-line commands (e.g., iw, d, :copen)\n",
    "4. Formats mathematical equations using LaTeX blocks ($ <math equation> $) without double backslashes (\\\\)\n",
    "\n",
    "# Requirements\n",
    "* The improved note should be factually equivalent to the original note.\n",
    "* Conciseness and formatting im4provements are expected.\n",
    "\n",
    "# Response Format\n",
    "Provide:\n",
    "* A concise reasoning for your evaluation\n",
    "* A boolean answer: True (improved note is properly formatted) or False (improved note is not properly formatted)\n",
    "\"\"\"\n",
    "\n",
    "judge_json2 = LLMJudgeJSON(\n",
    "    model_name=MODEL_NAME,\n",
    "    chat=chat,\n",
    "    system_msg=SYSTEM_MSG_V7,\n",
    "    user_msg_tmpl=user_msg_tmpl_2,\n",
    "    review_model=Review2,\n",
    "    orig_deck=orig_deck,\n",
    ")\n",
    "review_notes(ra=ra, judge=judge_json2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8488409e-9346-46ad-9635-e78388d8c71d",
   "metadata": {},
   "source": [
    "### Back to the Basics\n",
    "\n",
    "It seems that our initial prompt was the best-performing one. Few-shot learning, reasoning, and even passing the original note did not help improve performance. Let's try to take another stab at prompt engineering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2771306d-bed4-4edd-bfbf-d4b3db424d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MSG_V8 = r\"\"\"You are a helpful assistant.\"\"\"\n",
    "\n",
    "user_msg_tmpl_3 = \"\"\"Your job is to evaluate Anki's notes and classify notes that are not formatted correctly.\n",
    "\n",
    "Requirements:\n",
    "* Only check formatting\n",
    "* Notes are written in hybrid markdown; for instance: the newline character is `<br>,` `<` is `&lt;`, etc.\n",
    "* Preserve images and media on the original note\n",
    "* Use code block: ```<language><br><command><br>```\n",
    "* Use inline code format for short commands: e.g., `iw`, `d`, etc.\n",
    "\n",
    "Provide only a boolean score: False for bad and True for good.\n",
    "\n",
    "Original note\n",
    "Front: {{ orig_note.front }}\n",
    "Back: {{ orig_note.back }}\n",
    "Tags: {{ orig_note.tags }}\n",
    "\n",
    "Improved note\n",
    "Front: {{ note.front }}\n",
    "Back: {{ note.back }}\n",
    "Tags: {{ note.tags }}\n",
    "\"\"\"\n",
    "\n",
    "judge_json = LLMJudgeJSON(\n",
    "    model_name=MODEL_NAME,\n",
    "    chat=chat,\n",
    "    system_msg=SYSTEM_MSG_V8,\n",
    "    user_msg_tmpl=user_msg_tmpl_3,\n",
    "    orig_deck=orig_deck,\n",
    ")\n",
    "review_notes(ra=ra, judge=judge_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017974df-4813-447e-8b53-ee6fae73c622",
   "metadata": {},
   "source": [
    "### Create helper functions to facilitate reviewing notes\n",
    "\n",
    "Let's create a `pandas.DataFrame` with both: original note, edited note, and LLM review. This will facilitate our review of the LLM reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dbc6e0-3656-4063-94b0-242b065ad501",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.read_csv(\"../data/eval.txt\", sep=\"\\t\", header=None)\n",
    "df_eval.columns = [\"guid\", \"score\"]\n",
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf1acc2-cd57-43bd-8eb5-ddc7751151b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ra._ReviewApp__reviews\n",
    "dict_data = [item.dict() for item in results]\n",
    "df_scores = pd.DataFrame(dict_data)\n",
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f345c24-71b9-4b66-9851-a41df05a967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [note.dict() for note in deck]\n",
    "df_notes = pd.DataFrame(a)\n",
    "df_notes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefe3d5f-5f7e-4004-a8be-0a041b18f87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.merge(df_notes, df_scores, how=\"inner\", on=\"guid\")\n",
    "x = x[x.tags.apply(lambda a: \"life\" not in a)]  # exclude personal notes\n",
    "print(x.shape)\n",
    "x.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67017da-94d2-41bf-b141-3f59f5caddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_interactive_session(session_text):\n",
    "    lines = session_text.strip().split(\"<br>\")\n",
    "    input_pattern = r\"^>>> .*$\"\n",
    "    continuation_pattern = r\"^... .*$\"\n",
    "    output_pattern = r\"^(?!>>>)(?!\\.\\.\\.)\"\n",
    "\n",
    "    state = \"expecting_input\"\n",
    "    for i, line in enumerate(lines, 1):\n",
    "        if state == \"expecting_input\":\n",
    "            if not (\n",
    "                re.match(input_pattern, line) or re.match(continuation_pattern, line)\n",
    "            ):\n",
    "                return False, f\"Line {i}: Expected input (>>> or ...), got: {line}\"\n",
    "            state = \"optional_output\"\n",
    "        elif state == \"optional_output\":\n",
    "            if re.match(input_pattern, line) or re.match(continuation_pattern, line):\n",
    "                state = \"expecting_input\"\n",
    "            elif not re.match(output_pattern, line):\n",
    "                return False, f\"Line {i}: Invalid output format: {line}\"\n",
    "\n",
    "    return True, \"Valid interactive session format\"\n",
    "\n",
    "\n",
    "def validate_code_block_format(block):\n",
    "    # Check if the block starts and ends with ```\n",
    "    if not (block.startswith(\"```\") and block.endswith(\"```\")):\n",
    "        return False, \"Code block should start and end with ```\"\n",
    "\n",
    "    # Remove the opening and closing ```\n",
    "    content = block[3:-3].strip()\n",
    "\n",
    "    # Check if the block starts with a language specifier\n",
    "    if not re.match(r\"^[\\w-]+<br>\", content):\n",
    "        return (\n",
    "            False,\n",
    "            \"Code block should start with a language specifier followed by <br>\",\n",
    "        )\n",
    "\n",
    "    # Split the content by <br> tags\n",
    "    lines = content.split(\"<br>\")\n",
    "\n",
    "    # Check if the last line is empty (as it should end with <br>)\n",
    "    if lines[-1].strip() != \"\":\n",
    "        return False, \"Code block should end with <br>\"\n",
    "\n",
    "    # Check if there are any empty lines in between (which would indicate missing <br>)\n",
    "    if any(line.strip() == \"\" for line in lines[1:-1]):\n",
    "        return (\n",
    "            False,\n",
    "            \"Code block should not have empty lines. Use <br> for line breaks.\",\n",
    "        )\n",
    "\n",
    "    return True, \"Valid code block format\"\n",
    "\n",
    "\n",
    "def validate_hybrid_markdown(content):\n",
    "    issues = []\n",
    "\n",
    "    # Check for double backslashes in LaTeX blocks\n",
    "    latex_blocks = re.findall(r\"\\$(.*?)\\$\", content, re.DOTALL)\n",
    "    for block in latex_blocks:\n",
    "        if \"\\\\\\\\\" in block:\n",
    "            issues.append(\n",
    "                \"Double backslash (\\\\\\\\) found in LaTeX block. This may cause rendering issues.\"\n",
    "            )\n",
    "\n",
    "    # Check for unmatched dollar signs\n",
    "    # Split the content into code blocks and non-code blocks\n",
    "    parts = re.split(r\"(```[\\s\\S]*?```)\", content)\n",
    "\n",
    "    total_dollar_count = 0\n",
    "    for part in parts:\n",
    "        if part.startswith(\"```\") and part.endswith(\"```\"):\n",
    "            # This is a code block\n",
    "            is_valid, message = validate_code_block_format(part)\n",
    "            if not is_valid:\n",
    "                issues.append(f\"Invalid code block format: {message}\")\n",
    "\n",
    "            if part.startswith(\"```python\"):\n",
    "                # Check if it's an interactive Python session\n",
    "                session_content = part[13:-3].strip()  # Remove ```python<br> and ```\n",
    "                is_valid, message = validate_interactive_session(session_content)\n",
    "                if not is_valid:\n",
    "                    issues.append(\n",
    "                        f\"Invalid Python interactive session in code block: {message}\"\n",
    "                    )\n",
    "        else:\n",
    "            # Count dollar signs in non-code block parts\n",
    "            dollar_count = part.count(\"$\")\n",
    "            total_dollar_count += dollar_count\n",
    "\n",
    "    # Check if the total number of dollar signs outside code blocks is odd\n",
    "    if total_dollar_count % 2 != 0:\n",
    "        issues.append(\n",
    "            \"Unmatched dollar signs outside code blocks. LaTeX may not render correctly.\"\n",
    "        )\n",
    "\n",
    "    # Check for common Markdown syntax errors\n",
    "    if \"```\" in content and content.count(\"```\") % 2 != 0:\n",
    "        issues.append(\n",
    "            \"Unmatched code block delimiters (```). Code blocks may not render correctly.\"\n",
    "        )\n",
    "\n",
    "    return issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de65941-7107-4a17-88cc-3590d0f1ca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reviews = 100\n",
    "\n",
    "for row in x.iloc[:n_reviews].iterrows():\n",
    "    note = row[1]\n",
    "    print(f\"Front: {note['front']}\\nBack: {note['back']}\\nTags: {note['tags']}\")\n",
    "    for side in [\"front\", \"back\"]:\n",
    "        a = note[side]\n",
    "        issues = validate_hybrid_markdown(a)\n",
    "        if issues:\n",
    "            for issue in issues:\n",
    "                print(f\"Issue {side}: {issue}\")\n",
    "        else:\n",
    "            print(f\"Issue {side}: None\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba9168c-46f6-49a7-851b-4b23a9669aff",
   "metadata": {},
   "source": [
    "Common errors are:\n",
    "\n",
    "* Missing `<img>`\n",
    "* Wrong prompt (e.g., `>>`, missing `$`)\n",
    "* Missing `<br>` inside code block\n",
    "* Missing `<br>` outside code block\n",
    "* `\\\\` in LaTeX\n",
    "* References (should we remove them?)\n",
    "* Trailing `.` (full stop)\n",
    "* Using code block for note that does not contain code\n",
    "* \"```bash\" for keymap\n",
    "* Missing language in code block\n",
    "* Unmatched code block delimiter (missing trailing \"```\")\n",
    "* Missing inline code block for keymap or short commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0fc84d-92e4-4f4a-a84f-cd96d9921e5b",
   "metadata": {},
   "source": [
    "### Todo\n",
    "\n",
    "- [ ] Create a dataset to measure LLM judge's alignment with human preference \n",
    "- [ ] Use _reflection_ agentic workflow to improve notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096944a4-fb5c-47b1-ac86-88ac99949927",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
