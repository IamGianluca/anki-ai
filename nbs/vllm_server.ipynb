{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad251cb3-a145-4cd6-9501-81fd4041f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2d6d6b9-3682-4599-ab8f-2d3f4d9f4d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ipython().system = os.system\n",
    "!vllm serve meta-llama/Meta-Llama-3.1-8B-Instruct --max_model_len 4096 --chat-template ../template_llama31.jinja &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87845dae-ed20-4a70-9a14-4dc11067b93b",
   "metadata": {},
   "source": [
    "### Using OpenAI Completions API with vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa5610db-8bc5-40f6-9a2f-8aeb53f99808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-29 08:21:41 logger.py:36] Received request cmpl-07ac87b1c0d548f89f624c79b3afd519-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 24661, 13175, 374, 264], lora_request: None, prompt_adapter_request: None.\n",
      "INFO 08-29 08:21:41 async_llm_engine.py:208] Added request cmpl-07ac87b1c0d548f89f624c79b3afd519-0.\n",
      "INFO 08-29 08:21:41 metrics.py:351] Avg prompt throughput: 0.7 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 08-29 08:21:41 async_llm_engine.py:176] Finished request cmpl-07ac87b1c0d548f89f624c79b3afd519-0.\n",
      "INFO:     127.0.0.1:34594 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "{\"id\":\"cmpl-07ac87b1c0d548f89f624c79b3afd519\",\"object\":\"text_completion\",\"created\":1724934101,\"model\":\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"text\":\" top tourist destination, and for good\",\"logprobs\":null,\"finish_reason\":\"length\",\"stop_reason\":null,\"prompt_logprobs\":null}],\"usage\":{\"prompt_tokens\":5,\"total_tokens\":12,\"completion_tokens\":7}}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   493  100   365  100   128   2270    796 --:--:-- --:--:-- --:--:--  3081\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!curl http://localhost:8000/v1/completions -H \"Content-Type: application/json\" -d '{     \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",   \"prompt\": \"San Francisco is a\",   \"max_tokens\": 7,  \"temperature\": 0 }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ff18703-c762-4b1e-8cc7-8f56a718aabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-29 08:21:45 logger.py:36] Received request cmpl-845255fe86bf458c9fc671cffa59e91c-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 24661, 13175, 374, 264], lora_request: None, prompt_adapter_request: None.\n",
      "INFO 08-29 08:21:45 async_llm_engine.py:208] Added request cmpl-845255fe86bf458c9fc671cffa59e91c-0.\n",
      "INFO 08-29 08:21:45 async_llm_engine.py:176] Finished request cmpl-845255fe86bf458c9fc671cffa59e91c-0.\n",
      "INFO:     127.0.0.1:55712 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "Completion result:  top tourist destination, and for good\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "completion = client.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    prompt=\"San Francisco is a\",\n",
    "    temperature=0,\n",
    "    max_tokens=7,\n",
    ")\n",
    "print(\"Completion result:\", completion.choices[0].text);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613bb08d-8661-4e51-9c78-8b3d200fa66c",
   "metadata": {},
   "source": [
    "### Using OpenAI Chat API with vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7cc64cd-3402-4786-ae08-c70dba342f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-29 08:23:53 logger.py:36] Received request chat-d9326d085f3749a8bec6176740b1001a: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is the result of 2 + 2?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4044, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 2675, 527, 264, 11190, 18328, 13, 128009, 128006, 882, 128007, 271, 3923, 374, 279, 1121, 315, 220, 17, 489, 220, 17, 30, 128009, 128006, 78191, 128007, 271], lora_request: None, prompt_adapter_request: None.\n",
      "INFO 08-29 08:23:53 async_llm_engine.py:208] Added request chat-d9326d085f3749a8bec6176740b1001a.\n",
      "INFO 08-29 08:23:53 metrics.py:351] Avg prompt throughput: 4.7 tokens/s, Avg generation throughput: 1.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 08-29 08:23:54 async_llm_engine.py:176] Finished request chat-d9326d085f3749a8bec6176740b1001a.\n",
      "INFO:     127.0.0.1:44482 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "Chat response: The result of 2 + 2 is 4.\n",
      "INFO 08-29 08:24:04 metrics.py:351] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 08-29 08:24:14 metrics.py:351] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 08-29 08:24:24 metrics.py:351] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 08-29 08:24:34 metrics.py:351] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 08-29 08:24:44 metrics.py:351] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "# Set OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the result of 2 + 2?\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "print(\"Chat response:\", chat_response.choices[0].message.content);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a4da63-66bd-46a4-a6cf-18d2f7c05298",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
